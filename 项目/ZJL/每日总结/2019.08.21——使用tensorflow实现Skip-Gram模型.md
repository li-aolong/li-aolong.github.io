# 使用tensorflow实现Skip-Gram模型

## 数据预处理

首先对文本的清洗和分词操作，替换文本中特殊符号并删除低频词。有了分词后的文本，进行构建映射表。

洗后的数据：

![img](https://pic2.zhimg.com/80/v2-b3d21fc34d62c03e3d9bf299024734f9_hd.png)

## 模型构建

在模型中为了加速训练并提高词向量的质量，采用负采样方式进行权重更新。

输入层到隐层的权重矩阵作为嵌入层要给定其维度，一般embeding_size设置为50-300之间。嵌入层的lookup通过TensorFlow中的embedding_lookup实现。

在skip-gram中，每个input word的多个上下文单词实际上是共享一个权重矩阵，将每个（input word, output word）训练样本来作为输入。为了加速训练并且提高词向量的质量，采用negative sampling的方法来进行权重更新。

## 训练样本

在skip-gram中，训练样本的形式是(input word, output word)，其中output word是input word的上下文。为了减少模型噪音并加速训练速度，在构造batch之前要对样本进行采样，剔除停用词等噪音因素。

- **采样**：在建模过程中，训练文本中会出现很多“the”、“a”之类的常用词（也叫停用词），这些词对于训练会带来很多噪音。通过剔除高频的停用词来减少模型的噪音，并加速训练。
- **构造batch**：skip-gram不同于CBOW，CBOW是基于上下文预测当前input word。而skip-gram则是基于一个input word来预测上下文，因此一个input word会对应多个上下文。因此转换为代码就是两个步骤，第一个是找到每个input word的上下文，第二个就是基于上下文构建batch。

## 结果

在上面的步骤中，已经将模型的框架搭建出来，为了能够更加直观地观察训练每个阶段的情况，挑选几个词，查看在训练过程中它们的相似词是怎么变化的。部分结果如下：

![img](https://pic1.zhimg.com/80/v2-359072c704253bca018fc00cc658e71c_hd.png)

**技巧**：

- 增大训练样本，语料库越大，模型学习的可学习的信息会越多
- 增加window size，可以获得更多的上下文信息
- 增加embedding size可以减少信息的维度损失，但不宜过大