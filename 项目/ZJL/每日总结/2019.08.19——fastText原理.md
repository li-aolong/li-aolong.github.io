# fastText原理

## 简介

fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：

1. fastText在保持高精度的情况下加快了训练速度和测试速度
2. fastText不需要预训练好的词向量，fastText会自己训练词向量
3. fastText两个重要的优化：Hierarchical Softmax、N-gram

## 模型架构

fastText模型架构和word2vec中的CBOW很相似， 不同之处是fastText预测标签而CBOW预测的是中间词，即模型架构类似但是模型的任务不同：

![image](https://note.youdao.com/yws/api/personal/file/D96181D8E0B5464495715A60EE33A4A4?method=download&shareKey=d366693a06d77aecd2c8c8e53786c8fc)

word2vec将上下文关系转化为多分类任务，进而训练逻辑回归模型，word2vec中提供了两种针对大规模多分类问题的优化手段， negative sampling 和hierarchical softmax。

fastText模型架构中x1,x2,…,xN−1,xN表示一个文本中的n-gram向量，每个特征是词向量的平均值。这与cbow相似，cbow用上下文去预测中心词，而此处用全部的n-gram去预测指定类别：

![image](https://note.youdao.com/yws/api/personal/file/A5695F25596D4311835B2DA25AE8269B?method=download&shareKey=f97031192717a3d0c76c80f649a4344e)

## 层次softmax

- 对于有大量类别的数据集，fastText使用了一个分层分类器（而非扁平式架构），不同的类别被整合进树形结构中
- 为了改善运行时间，fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在哈弗曼编码的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。
- fastText 也利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构
- 频繁出现类别的树形结构的深度要比不频繁出现类别的树形结构的深度要小，这也使得进一步的计算效率更高
- ![11](http://www.datagrand.com/blog/wp-content/uploads/2018/01/11-1024x337.jpg)

## N-gram子词特征

n-gram是基于语言模型的算法，基本思想是将文本内容按照子节顺序进行大小为N的窗口滑动操作，最终形成窗口为N的字节片段序列。

n-gram可以根据粒度不同有不同的含义，有字粒度的n-gram和词粒度的n-gram。

使用n-gram有如下优点：

- 为罕见的单词生成更好的单词向量
- 即使单词没有出现在训练语料库中，仍然可以从字符级n-gram中构造单词的词向量
- 让模型学习到局部单词顺序的部分信息

## 与word2vec的异同

相似处：

- 图模型结构很像，都是采用embedding向量的形式，得到word的隐向量表达
- 都采用很多相似的优化方法，如Hierarchical softmax

不同处：

- 模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是分类的label
- 模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext 对应的整个sentence的内容，包括term，也包括 n-gram的内容
- 本质区别在于h-softmax的使用：word2vec的目的是得到词向量；fastText则充分利用了h-softmax的分类功能





