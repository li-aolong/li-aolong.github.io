[TOC]

# Skip-Gram模型

## Word2Vec和Embeddings

- Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，Word2Vec是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。
- Embedding就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。

## 模型

- Word2Vec主要有Skip-Gram和CBOW两种模型，直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word
- ![preview](https://pic1.zhimg.com/v2-35339b4e3efc29326bad70728e2f469c_r.jpg)
- Skip-Gram模型分为了两个部分，**第一部分为建立模型，第二部分是通过模型获取嵌入词向量**
- Skip-Gram的建模过程与自编码器（auto-encoder）的思想相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，不会用这个训练好的模型处理新的任务，而真正需要的是这个模型通过训练数据所学得的参数，如隐层的权重矩阵

## 细节

- 首先基于训练文档构建词汇表，再对单词进行one-hot编码
- ![img](https://pic2.zhimg.com/v2-5c16aa8eaa670485ed5cbcd68e4b8b41_b.png)
- 隐层没有使用任何激活函数，但是输出层使用了sotfmax
- 基于成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word和output word都是one-hot编码的向量，最终模型的输出是一个概率分布

## 隐层

- ![img](https://pic1.zhimg.com/v2-c538566f7d627ce7ca40589f15ca8284_b.png)
- 左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行代表了每个单词的词向量
- 所以最终的目标就是学习这个隐层的权重矩阵
- 模型中的隐层权重矩阵便成了一个”查找表“（lookup table），进行矩阵计算时，直接去查输入向量中取值为1的维度下对应的那些权重值，隐层的输出就是每个输入单词的“嵌入词向量”。

## 输出层

- 输出层是一个softmax回归分类器，它的每个结点将会输出一个0-1之间的值（概率），这些所有输出层神经元结点的概率之和为1
- 训练样本为 (input word: “ants”， output word: “car”) 的计算示意图如下所示
- ![preview](https://pic4.zhimg.com/v2-4bfcb484d9353ef612c1a6154f0c3077_r.jpg)

## 优化

- **单词组合**：一些单词组合（或者词组）的含义和拆开以后具有完全不同的意义
- **对高频词抽样**：对于在训练原始文本中遇到的每一个单词，它们都有一定概率被从文本中删掉，而这个被删除的概率与单词的频率有关
- **负采样**：
  - vocabulary的大小决定了Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢
  - 负采样是用来提高训练速度并且改善所得到词向量的质量的一种方法
  - 负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量