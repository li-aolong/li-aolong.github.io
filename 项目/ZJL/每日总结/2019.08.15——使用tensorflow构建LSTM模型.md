# 使用tensorflow构建LSTM模型

## 循环神经网络（RNNs）

RNNs的网络有一个循环的操作，使得它们能够保留之前学习到的内容，但无法解决长时期的依赖关系

![An unrolled recurrent neural network.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)

## LSTM网络

长短期记忆网络（LSTMs）是RNN 中一个特殊的类型，能够记住很长时期内的信息

![A LSTM neural network.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)



## 模型构建

### 数据预处理

首先进行数据的加载与编码转换，基于字符进行模型构建，然后划分数据集，使用mini-batch来进行模型训练。

### 模块构建

- 输入层：定义inputs与targets，为了减少过拟合加入dropout
- LSTM层：LSTM层是整个神经网络的关键部分，TensorFlow中，tf.contrib.rnn模块中有BasicLSTMCell和LSTMCell两个包，使用tf.contrib.rnn.MultiRNNCell实现对基本LSTM cell的顺序堆叠，它接收的是cell对象组成的list。最后initial_state定义了初始cell state
- 输出层：到目前为止，输入和LSTM层都已经构建完毕。输出层采用softmax，它与LSTM进行全连接。将数据重塑后，对LSTM层和softmax层进行连接。并计算logits和softmax后的概率分布。
- 训练误差计算：至此已经完成了整个网络的构建，接下来要定义train loss和optimizer。从sotfmax层输出的是概率分布，因此要对targets进行one-hot编码。采用tf.nn.softmax_cross_entropy_with_logits交叉熵来计算loss
- Optimizer：RNN会遇到梯度爆炸（gradients exploding）和梯度弥散（gradients disappearing)的问题。LSTM解决了梯度弥散的问题，但是仍然可能会爆炸，因此采用gradient clippling的方式来防止梯度爆炸。
- 使用tf.nn.dynamic_run来运行RNN序列

- 设置超参数：batch_size，num_steps，lstm_size，num_layers，learning_rate，keep_prob

## 文本生成

- 经过训练得到了一系列训练过程中保存下来的参数，可以利用这些参数来进行文本生成
- 了减少噪音，每次的预测值选择最可能的前5个进行随机选择
- ![img](https://pic4.zhimg.com/80/v2-417e3c80e63e2edf66e1941b60aab8a3_hd.png)