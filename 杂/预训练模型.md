## 预训练相关

- [从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)

## XLNet

- [XLNet:运行机制及和Bert的异同比较](<https://zhuanlan.zhihu.com/p/70257427>)
- [香侬读 | XLnet：比Bert更强大的预训练模型](<https://zhuanlan.zhihu.com/p/71759544>)
- [Transformer, Transformer-XL, XLNet: 追溯XLNet的前世今生](<https://blog.csdn.net/weixin_43269174/article/details/94323036>)
- [XLNet原理浅析](https://zhuanlan.zhihu.com/p/70395238)

## Attention

- [深度学习中的注意力模型（2017版）](<https://zhuanlan.zhihu.com/p/37601161>)
- [NLP进化史系列之语言模型](<https://blog.csdn.net/zengNLP/article/details/93904473>)
- [Attention Is All You Need(注意力模型）](https://zhuanlan.zhihu.com/p/44731789)

## 基准

- GLUE
- SQuAD
- RACE

## 模型

- bert：谷歌
- GPT：openAI
- GPT2：openAI
- transformer-XL：谷歌、CMU
- XLNet：谷歌、CMU
- XLM：facebook
- RoBert
- AlBert
- ELECTRA
- T5

